<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>llm — trucs.ai</title>
  <meta name="description" content="Run an LLM in your browser via WebGPU. No server, no API key.">
  <link rel="stylesheet" href="../assets/style.css">
  <style>
    button {
      font-family: inherit;
      font-size: 1rem;
      padding: 0.5em 1.5em;
      border: 1px solid #111;
      background: #fff;
      color: #111;
      cursor: pointer;
      border-radius: 2px;
    }
    button:hover {
      background: #111;
      color: #fff;
    }
    button:disabled {
      opacity: 0.4;
      cursor: not-allowed;
    }
    button:disabled:hover {
      background: #fff;
      color: #111;
    }
    #model-status {
      display: flex;
      align-items: center;
      gap: 0.75em;
    }
    #status-text {
      color: #555;
      font-size: 0.85rem;
    }
    textarea {
      font-family: inherit;
      font-size: 1rem;
      width: 100%;
      min-height: 4em;
      padding: 0.5em;
      border: 1px solid #ddd;
      background: #fff;
      color: #111;
      resize: vertical;
      line-height: 1.6;
      border-radius: 2px;
    }
    textarea:focus {
      outline: none;
      border-color: #111;
    }
    .controls {
      display: flex;
      align-items: center;
      gap: 1em;
      margin-top: 0.75em;
    }
    #output {
      margin-top: 1.5em;
      white-space: pre-wrap;
      word-break: break-word;
      min-height: 4em;
    }
    #stats {
      margin-top: 0.5em;
      color: #999;
      font-size: 0.8rem;
    }
  </style>
</head>
<body>
  <main>
    <h1>llm</h1>
    <p>
      A small language model running entirely in your browser via WebGPU.<br>
      No server, no API key. Your prompt never leaves your machine.<br>
      SmolLM2-1.7B, single-turn chat.
    </p>

    <div id="model-status">
      <button id="load-btn">download</button>
      <span id="status-text"></span>
    </div>

    <hr>

    <textarea id="input" placeholder="Say something..." maxlength="2000"></textarea>
    <div class="controls">
      <button id="send-btn" disabled>send</button>
    </div>

    <div id="output"></div>
    <div id="stats"></div>

    <hr>

    <p>Using <a href="https://github.com/mlc-ai/web-llm">WebLLM</a></p>

    <hr>
    <p><a href="/">← trucs.ai</a></p>
  </main>

  <script type="module">
    import { CreateMLCEngine } from 'https://esm.run/@mlc-ai/web-llm';

    const input = document.getElementById('input');
    const sendBtn = document.getElementById('send-btn');
    const loadBtn = document.getElementById('load-btn');
    const statusText = document.getElementById('status-text');
    const output = document.getElementById('output');
    const stats = document.getElementById('stats');

    let engine = null;
    let isCached = false;

    // Check if model is cached
    try {
      const keys = await caches.keys();
      for (const key of keys) {
        const cache = await caches.open(key);
        const matched = await cache.keys();
        if (matched.some(r => r.url.includes('SmolLM2'))) {
          isCached = true;
          loadBtn.textContent = 'load';
          break;
        }
      }
    } catch (_) {}
    statusText.textContent = isCached
      ? 'model cached locally: ~1GB'
      : 'model will be stored locally: ~1GB';

    if (!navigator.gpu) {
      statusText.textContent = 'WebGPU is not supported in this browser.';
      loadBtn.disabled = true;
    }

    async function loadModel() {
      loadBtn.disabled = true;
      statusText.textContent = 'loading...';

      try {
        engine = await CreateMLCEngine('SmolLM2-1.7B-Instruct-q4f16_1-MLC', {
          initProgressCallback: (report) => {
            const pct = (report.progress * 100).toFixed(0);
            const mbMatch = report.text.match(/([\d.]+)\s*MB/);
            const mb = mbMatch ? `${Math.round(parseFloat(mbMatch[1]))}MB loaded, ` : '';
            const verb = isCached ? 'Loading' : 'Downloading';
            statusText.textContent = `${verb}: ${mb}${pct}%`;
          },
        });

        statusText.textContent = 'ready';
        sendBtn.disabled = false;
      } catch (err) {
        statusText.textContent = 'failed to load: ' + err.message;
        loadBtn.disabled = false;
        console.error(err);
      }
    }

    async function generate() {
      const text = input.value.trim().slice(0, 2000);
      if (!text || !engine) return;

      sendBtn.disabled = true;
      output.textContent = '';
      stats.textContent = '';

      const start = performance.now();

      try {
        const chunks = await engine.chat.completions.create({
          messages: [{ role: 'user', content: text }],
          max_tokens: 512,
          stream: true,
        });

        let tokens = 0;
        for await (const chunk of chunks) {
          const delta = chunk.choices[0]?.delta?.content;
          if (delta) {
            output.textContent += delta;
            tokens++;
          }
        }

        const elapsed = (performance.now() - start) / 1000;
        const tokSec = (tokens / elapsed).toFixed(1);
        stats.textContent = `${tokens} tokens in ${elapsed.toFixed(1)}s (${tokSec} tok/s)`;
      } catch (err) {
        output.textContent = 'error: ' + err.message;
        console.error(err);
      }

      await engine.resetChat();
      sendBtn.disabled = false;
    }

    if (isCached) {
      loadModel();
    } else {
      loadBtn.addEventListener('click', loadModel);
    }
    sendBtn.addEventListener('click', generate);
    input.addEventListener('keydown', (e) => {
      if (e.key === 'Enter' && !e.shiftKey && !sendBtn.disabled) {
        e.preventDefault();
        generate();
      }
    });
  </script>
</body>
</html>
